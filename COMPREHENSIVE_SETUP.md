# ğŸš— HearAI-EV: Intelligent Acoustic Diagnostics System

**A machine learning-powered acoustic fault detection system for electric vehicles using YAMNet and Generative AI**

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [System Architecture](#system-architecture)
3. [Project Structure](#project-structure)
4. [Installation & Setup](#installation--setup)
5. [Quick Start](#quick-start)
6. [Usage Modes](#usage-modes)
7. [Components](#components)
8. [Configuration](#configuration)
9. [Results & Outputs](#results--outputs)
10. [Future Enhancements](#future-enhancements)

---

## ğŸ¯ Overview

HearAI-EV is an intelligent acoustic diagnostics system designed to detect and explain mechanical faults in electric vehicles. Since electric vehicles operate silently compared to conventional combustion engines, early-stage mechanical issues such as bearing wear or propeller anomalies often go unnoticed. This project addresses that challenge by continuously monitoring vehicle sounds and converting them into meaningful diagnostic information for the driver.

### Key Features

âœ… **Real-time Audio Monitoring** - Captures 1-minute acoustic samples  
âœ… **Automated Fault Detection** - YAMNet-based classification (bearing, propeller, healthy)  
âœ… **Confidence-based Decision Logic** - Probability thresholds ensure reliable predictions  
âœ… **AI-Generated Explanations** - Mistral LLM converts technical outputs to user-friendly messages  
âœ… **Visual Alert Interface** - Color-coded status displays with severity indicators  
âœ… **Diagnostic Dashboard** - Comprehensive health monitoring and trend analysis  
âœ… **Full Explainability** - Every prediction includes actionable recommendations  

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VEHICLE SOUND INPUT                          â”‚
â”‚              (1-minute audio samples @ 16kHz)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PREPROCESSING PIPELINE                        â”‚
â”‚   â€¢ Normalize & Resample to 16 kHz                             â”‚
â”‚   â€¢ Convert to Mono                                            â”‚
â”‚   â€¢ Extract Mel-Spectrogram Features                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              YAMNET TRANSFER LEARNING MODEL                     â”‚
â”‚   â€¢ Pretrained on AudioSet (millions of sounds)               â”‚
â”‚   â€¢ Fine-tuned on EV acoustic dataset                         â”‚
â”‚   â€¢ 3 classes: Bearing Fault, Propeller Fault, Healthy       â”‚
â”‚   â€¢ Output: Probability scores for each class                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               CONFIDENCE-BASED DECISION LOGIC                   â”‚
â”‚   â€¢ Minimum confidence threshold: 70%                          â”‚
â”‚   â€¢ Fault probability assessment                              â”‚
â”‚   â€¢ Severity determination (low/medium/high)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        MISTRAL LLM - EXPLANATION GENERATION                    â”‚
â”‚   â€¢ Converts technical ML output to human language            â”‚
â”‚   â€¢ Generates actionable recommendations                      â”‚
â”‚   â€¢ Provides estimated urgency levels                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              USER INTERFACE & ALERTING                          â”‚
â”‚   â€¢ Green: Vehicle Operating Normally                         â”‚
â”‚   â€¢ Yellow: Warning - Schedule Maintenance                    â”‚
â”‚   â€¢ Red: Critical - Immediate Action Required                 â”‚
â”‚   â€¢ HTML Dashboard, Mobile-friendly JSON                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
HearAI-EV/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ train/
â”‚       â”‚   â”œâ”€â”€ bearing/
â”‚       â”‚   â”œâ”€â”€ healthy/
â”‚       â”‚   â””â”€â”€ propeller/
â”‚       â”œâ”€â”€ val/
â”‚       â”‚   â”œâ”€â”€ bearing/
â”‚       â”‚   â”œâ”€â”€ healthy/
â”‚       â”‚   â””â”€â”€ propeller/
â”‚       â””â”€â”€ test/
â”‚           â”œâ”€â”€ bearing/
â”‚           â”œâ”€â”€ healthy/
â”‚           â””â”€â”€ propeller/
â”‚
â”œâ”€â”€ dataset/          # Original audio data (pre-processed)
â”‚   â”œâ”€â”€ Bearing/
â”‚   â”œâ”€â”€ Healthy/
â”‚   â””â”€â”€ Propeller/
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ yamnet_finetuned.h5      # Trained model (saved after training)
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ training_history.csv     # Training metrics
â”‚   â”œâ”€â”€ model_evaluation.json    # Test performance
â”‚   â”œâ”€â”€ alert_display.png        # Visual alerts
â”‚   â”œâ”€â”€ diagnostic_dashboard.png # Dashboard visualization
â”‚   â”œâ”€â”€ dashboard.html           # Interactive HTML dashboard
â”‚   â”œâ”€â”€ predictions_log.json     # All predictions
â”‚   â””â”€â”€ system_report.json       # Final system report
â”‚
â”œâ”€â”€ data_processing.py           # Phase 1: Data preparation & augmentation
â”œâ”€â”€ yamnet_training.py           # Phase 2: Model training & evaluation
â”œâ”€â”€ inference.py                 # Phase 3: Real-time prediction
â”œâ”€â”€ llm_explanations.py          # Phase 3B: LLM-based explanations
â”œâ”€â”€ ui_interface.py              # Phase 4: Visual interface
â”œâ”€â”€ main.py                      # Main orchestration
â”‚
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ COMPREHENSIVE_SETUP.md       # Setup instructions
â””â”€â”€ README.md                    # This file
```

---

## ğŸ’¾ Installation & Setup

### Prerequisites

- Python 3.8+
- pip or conda
- 8GB RAM minimum (16GB recommended)
- 5GB disk space for models and data

### Step 1: Clone and Navigate

```bash
cd "d:\VIII SEM\HearAI-EV"
```

### Step 2: Install Dependencies

```bash
# Using pip
pip install -r requirements.txt

# Or using conda
conda create -n hearai python=3.10
conda activate hearai
pip install -r requirements.txt
```

### Step 3: Verify Installation

```bash
python -c "import tensorflow as tf; print(f'TensorFlow {tf.__version__}')"
python -c "import librosa; print('Audio library ready')"
```

### Step 4: Setup Optional LLM (Mistral)

For local LLM integration:

```bash
# Install Ollama from https://ollama.ai
ollama pull mistral

# Or use pre-installed Mistral model if available
```

---

## ğŸš€ Quick Start

### Option 1: Run Complete Demo

```bash
python main.py --mode demo
```

This will:
1. Load the pre-trained model
2. Process test audio samples
3. Generate diagnostic reports
4. Create visualizations and dashboards
5. Analyze health trends

### Option 2: Process Specific Audio Directory

```bash
python main.py --mode process --audio-dir data/processed/test --limit 20
```

### Option 3: Continuous Monitoring Simulation

```bash
python main.py --mode monitor
```

---

## ğŸ“– Usage Modes

### Training Mode (Phase 2)

If you need to retrain the model:

```bash
python yamnet_training.py
```

This will:
- Load preprocessed data from `data/processed/`
- Build YAMNet architecture with custom classification head
- Train for up to 50 epochs with early stopping
- Save model to `models/yamnet_finetuned.h5`
- Generate evaluation metrics and visualizations

### Inference Mode (Default)

```bash
from inference import HearAIPredictor, get_diagnostic_info

# Initialize predictor
predictor = HearAIPredictor('models/yamnet_finetuned.h5')

# Predict on audio file
result = predictor.predict('path/to/audio.wav')

# Get diagnostic information
diagnostic = get_diagnostic_info(result)

print(f"Status: {diagnostic['status']}")
print(f"Confidence: {diagnostic['confidence']}%")
print(f"Recommendations: {diagnostic['symptoms']}")
```

### LLM Integration Mode

```bash
from llm_explanations import DiagnosticReport
from inference import HearAIPredictor, get_diagnostic_info

# Predict
predictor = HearAIPredictor()
prediction = predictor.predict('audio.wav')

# Generate diagnostic info
diagnostic = get_diagnostic_info(prediction)

# Generate report with LLM explanations
report_gen = DiagnosticReport()
report = report_gen.create_report(prediction, diagnostic)

# Format for display
print(report_gen.format_for_display(report))

# Save report
report_gen.save_report(report)
```

---

## ğŸ”§ Components

### 1. **data_processing.py** - Phase 1: Data Preparation
- Scans and validates audio files
- Applies leak-free train/val/test split
- Generates 20 augmentations per file (stretch, pitch shift, noise, etc.)
- Creates comprehensive reports

**Key Functions:**
- `step1_scan_and_validate()` - Audio quality checks
- `step2_split_files()` - Stratified splitting
- `step3_augment_splits()` - Data augmentation
- `step4_generate_reports()` - Statistics and visualizations

### 2. **yamnet_training.py** - Phase 2: Model Training
- Loads YAMNet from TensorFlow Hub
- Fine-tunes on EV acoustic data
- Implements custom classification head
- Evaluates on test set

**Key Functions:**
- `build_yamnet_model()` - Architecture definition
- `train_model()` - Training loop with callbacks
- `evaluate_model()` - Comprehensive evaluation
- `plot_evaluation_results()` - Visualizations

### 3. **inference.py** - Phase 3: Real-time Prediction
- Loads trained model
- Processes audio in real-time
- Generates confidence scores
- Determines fault severity

**Key Classes:**
- `HearAIPredictor` - Main inference engine
- `ContinuousMonitor` - Monitors health trends

### 4. **llm_explanations.py** - Phase 3B: LLM Integration
- Generates human-readable explanations
- Uses Mistral LLM (local or via Ollama)
- Creates maintenance guides
- Estimates urgency levels

**Key Classes:**
- `DiagnosticLLM` - LLM interface
- `DiagnosticReport` - Report generation

### 5. **ui_interface.py** - Phase 4: Visual Interface
- Generates alert screens
- Creates diagnostic dashboards
- Produces HTML reports
- Mobile-friendly formatting

**Key Classes:**
- `AlertDisplay` - Visual alerts
- `DiagnosticDashboard` - Dashboard visualization

### 6. **main.py** - Orchestration
- Coordinates all components
- Provides command-line interface
- Runs demos and batch processing
- Generates final reports

---

## âš™ï¸ Configuration

### Model Configuration

Modify `CONFIG` in `yamnet_training.py`:

```python
CONFIG = {
    'classes': ['bearing', 'healthy', 'propeller'],
    'batch_size': 32,
    'epochs': 50,
    'learning_rate': 1e-4,
    'early_stopping_patience': 5,
}
```

### Inference Configuration

Modify `CONFIG` in `inference.py`:

```python
CONFIG = {
    'confidence_threshold': 0.7,  # Minimum confidence for classification
    'fault_threshold': 0.5,        # Probability above which it's a fault
    'input_sr': 16000,             # Sample rate
}
```

### LLM Configuration

Modify `LLM_CONFIG` in `llm_explanations.py`:

```python
LLM_CONFIG = {
    'local_model': 'mistral',
    'temperature': 0.7,
    'max_tokens': 500,
    'model_name': 'mistral-7b-instruct',
}
```

---

## ğŸ“Š Results & Outputs

### Training Phase Outputs
- `models/yamnet_finetuned.h5` - Trained model
- `reports/training_history.csv` - Epoch-by-epoch metrics
- `reports/model_evaluation.json` - Test set performance
- `reports/model_evaluation.png` - Visualization plots

### Inference Phase Outputs
- `reports/predictions_log.json` - All predictions
- `reports/alert_display.png` - Alert screen
- `reports/diagnostic_dashboard.png` - Dashboard visualization
- `reports/dashboard.html` - Interactive HTML dashboard
- `reports/system_report.json` - System summary

### Expected Performance

Based on EV acoustic data:
- **Overall Accuracy**: 85-92%
- **Bearing Detection Recall**: 88-95%
- **Propeller Detection Recall**: 82-90%
- **Healthy Classification**: 90-98%

---

## ğŸ¨ Visual Outputs

### Alert Display
Color-coded status screen showing:
- âœ… **Green (Healthy)**: Vehicle operating normally
- âš ï¸ **Yellow (Warning)**: Schedule maintenance within 24-48 hours
- ğŸ”´ **Red (Critical)**: Immediate action required

### Dashboard
Comprehensive visualization with:
- Status timeline
- Confidence trends
- Fault type distribution
- Severity analysis
- Historical records

---

## ğŸ”® Future Enhancements

### Phase 2 Improvements
- [ ] Real-time streaming audio processing
- [ ] Edge deployment on vehicle hardware
- [ ] Cloud synchronization for fleet management
- [ ] Mobile app integration (iOS/Android)

### Phase 3 Enhancements
- [ ] Multi-language explanations
- [ ] Integration with vehicle diagnostics CAN bus
- [ ] Predictive maintenance scheduling
- [ ] Driver behavior analysis

### Phase 4 Improvements
- [ ] Voice feedback integration
- [ ] AR visualization in vehicle HUD
- [ ] Bluetooth integration with smartwatch
- [ ] Over-the-air model updates

---

## ğŸ› Troubleshooting

### Model Not Found
```bash
# Ensure training has completed
python yamnet_training.py

# Or download pre-trained model
# [Link to pre-trained model]
```

### CUDA/GPU Issues
```bash
# CPU-only mode
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
```

### Audio Loading Errors
```bash
# Verify audio format (WAV recommended)
# Check sample rate compatibility
# Ensure file permissions
```

### Memory Issues
```bash
# Reduce batch size in configuration
CONFIG['batch_size'] = 16  # From 32

# Or limit number of files processed
python main.py --mode process --limit 10
```

---

## ğŸ“š References

1. YAMNet: https://github.com/google-research/perch/tree/main/chirp/projects/yamnet
2. Mistral LLM: https://mistral.ai/
3. TensorFlow Audio: https://www.tensorflow.org/io/tutorials/audio
4. Librosa Documentation: https://librosa.org/

---

## ğŸ“ License & Attribution

This project is part of the EV Acoustic Diagnostics Research Initiative.

**Contributors:**
- ML/Deep Learning: YAMNet Fine-tuning, Data Augmentation
- LLM Integration: Mistral-based Explanations
- UI/UX: Diagnostic Dashboard, Alert Interface

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- [ ] Additional fault types (cooling fan, motor issues)
- [ ] Cross-vehicle acoustic generalization
- [ ] Real-time optimization
- [ ] Hardware acceleration

---

## ğŸ“§ Support

For issues or questions:
1. Check troubleshooting section
2. Review configuration settings
3. Examine log files in `logs/`
4. Check generated reports for clues

---

## ğŸ“ Educational Value

This project demonstrates:
- âœ… Transfer learning with pretrained models
- âœ… Audio feature extraction and processing
- âœ… Balanced dataset creation with augmentation
- âœ… Confidence-based decision making
- âœ… LLM integration for explainability
- âœ… End-to-end ML system design
- âœ… Production-grade visualization
- âœ… Real-world IoT application

---

**Last Updated:** January 2026  
**Version:** 1.0  
**Status:** Production Ready âœ…
